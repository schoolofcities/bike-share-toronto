{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregating data for D3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>closed_status</th>\n",
       "      <th>duration</th>\n",
       "      <th>start_id</th>\n",
       "      <th>start_date</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>end_date</th>\n",
       "      <th>end_id</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>rider_type</th>\n",
       "      <th>...</th>\n",
       "      <th>start_time_only</th>\n",
       "      <th>start_time_seconds</th>\n",
       "      <th>end_date_only</th>\n",
       "      <th>end_time_only</th>\n",
       "      <th>end_time_seconds</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lon</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29681508</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>62</td>\n",
       "      <td>7948</td>\n",
       "      <td>2024-06-30 03:16:00</td>\n",
       "      <td>Bathurst St / Housey St</td>\n",
       "      <td>2024-06-30 03:17:00</td>\n",
       "      <td>7948</td>\n",
       "      <td>Bathurst St / Housey St</td>\n",
       "      <td>MEMBER</td>\n",
       "      <td>...</td>\n",
       "      <td>03:16:00</td>\n",
       "      <td>11760</td>\n",
       "      <td>2024-06-30</td>\n",
       "      <td>03:17:00</td>\n",
       "      <td>11822</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>43.637819</td>\n",
       "      <td>-79.400132</td>\n",
       "      <td>43.637819</td>\n",
       "      <td>-79.400132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29682786</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>876</td>\n",
       "      <td>7948</td>\n",
       "      <td>2024-06-30 08:18:00</td>\n",
       "      <td>Bathurst St / Housey St</td>\n",
       "      <td>2024-06-30 08:32:00</td>\n",
       "      <td>7938</td>\n",
       "      <td>Portland St / Wellington St W</td>\n",
       "      <td>MEMBER</td>\n",
       "      <td>...</td>\n",
       "      <td>08:18:00</td>\n",
       "      <td>29880</td>\n",
       "      <td>2024-06-30</td>\n",
       "      <td>08:32:00</td>\n",
       "      <td>30756</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>43.637819</td>\n",
       "      <td>-79.400132</td>\n",
       "      <td>43.642902</td>\n",
       "      <td>-79.399370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29644869</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>766</td>\n",
       "      <td>7948</td>\n",
       "      <td>2024-06-28 16:04:00</td>\n",
       "      <td>Bathurst St / Housey St</td>\n",
       "      <td>2024-06-28 16:16:00</td>\n",
       "      <td>7927</td>\n",
       "      <td>Strachan Ave / East Liberty St - SMART</td>\n",
       "      <td>MEMBER</td>\n",
       "      <td>...</td>\n",
       "      <td>16:04:00</td>\n",
       "      <td>57840</td>\n",
       "      <td>2024-06-28</td>\n",
       "      <td>16:16:00</td>\n",
       "      <td>58606</td>\n",
       "      <td>Friday</td>\n",
       "      <td>43.637819</td>\n",
       "      <td>-79.400132</td>\n",
       "      <td>43.639065</td>\n",
       "      <td>-79.410810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29672434</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>1333</td>\n",
       "      <td>7948</td>\n",
       "      <td>2024-06-29 18:39:00</td>\n",
       "      <td>Bathurst St / Housey St</td>\n",
       "      <td>2024-06-29 19:02:00</td>\n",
       "      <td>7927</td>\n",
       "      <td>Strachan Ave / East Liberty St - SMART</td>\n",
       "      <td>MEMBER</td>\n",
       "      <td>...</td>\n",
       "      <td>18:39:00</td>\n",
       "      <td>67140</td>\n",
       "      <td>2024-06-29</td>\n",
       "      <td>19:02:00</td>\n",
       "      <td>68473</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>43.637819</td>\n",
       "      <td>-79.400132</td>\n",
       "      <td>43.639065</td>\n",
       "      <td>-79.410810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29691472</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>1002</td>\n",
       "      <td>7948</td>\n",
       "      <td>2024-06-30 14:07:00</td>\n",
       "      <td>Bathurst St / Housey St</td>\n",
       "      <td>2024-06-30 14:24:00</td>\n",
       "      <td>7802</td>\n",
       "      <td>King St W / Jameson Ave - SMART</td>\n",
       "      <td>MEMBER</td>\n",
       "      <td>...</td>\n",
       "      <td>14:07:00</td>\n",
       "      <td>50820</td>\n",
       "      <td>2024-06-30</td>\n",
       "      <td>14:24:00</td>\n",
       "      <td>51822</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>43.637819</td>\n",
       "      <td>-79.400132</td>\n",
       "      <td>43.637358</td>\n",
       "      <td>-79.436180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id closed_status  duration  start_id           start_date  \\\n",
       "0  29681508        NORMAL        62      7948  2024-06-30 03:16:00   \n",
       "1  29682786        NORMAL       876      7948  2024-06-30 08:18:00   \n",
       "2  29644869        NORMAL       766      7948  2024-06-28 16:04:00   \n",
       "3  29672434        NORMAL      1333      7948  2024-06-29 18:39:00   \n",
       "4  29691472        NORMAL      1002      7948  2024-06-30 14:07:00   \n",
       "\n",
       "        start_station_name             end_date  end_id  \\\n",
       "0  Bathurst St / Housey St  2024-06-30 03:17:00    7948   \n",
       "1  Bathurst St / Housey St  2024-06-30 08:32:00    7938   \n",
       "2  Bathurst St / Housey St  2024-06-28 16:16:00    7927   \n",
       "3  Bathurst St / Housey St  2024-06-29 19:02:00    7927   \n",
       "4  Bathurst St / Housey St  2024-06-30 14:24:00    7802   \n",
       "\n",
       "                         end_station_name rider_type  ... start_time_only  \\\n",
       "0                 Bathurst St / Housey St     MEMBER  ...        03:16:00   \n",
       "1           Portland St / Wellington St W     MEMBER  ...        08:18:00   \n",
       "2  Strachan Ave / East Liberty St - SMART     MEMBER  ...        16:04:00   \n",
       "3  Strachan Ave / East Liberty St - SMART     MEMBER  ...        18:39:00   \n",
       "4         King St W / Jameson Ave - SMART     MEMBER  ...        14:07:00   \n",
       "\n",
       "  start_time_seconds  end_date_only  end_time_only end_time_seconds  \\\n",
       "0              11760     2024-06-30       03:17:00            11822   \n",
       "1              29880     2024-06-30       08:32:00            30756   \n",
       "2              57840     2024-06-28       16:16:00            58606   \n",
       "3              67140     2024-06-29       19:02:00            68473   \n",
       "4              50820     2024-06-30       14:24:00            51822   \n",
       "\n",
       "  day_of_week  start_lat  start_lon    end_lat    end_lon  \n",
       "0      Sunday  43.637819 -79.400132  43.637819 -79.400132  \n",
       "1      Sunday  43.637819 -79.400132  43.642902 -79.399370  \n",
       "2      Friday  43.637819 -79.400132  43.639065 -79.410810  \n",
       "3    Saturday  43.637819 -79.400132  43.639065 -79.410810  \n",
       "4      Sunday  43.637819 -79.400132  43.637358 -79.436180  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "df = pd.read_csv('data/ridership.csv')\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duration histogram data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/69/g590bg750s935v88zgfrdm9w0000gn/T/ipykernel_60084/3535152496.py:13: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  duration_counts = df_filtered.groupby(['duration_bin', 'bike_model']).size().unstack(fill_value=0)\n"
     ]
    }
   ],
   "source": [
    "# Filtering for trips less n seconds\n",
    "max_duration = 3600 \n",
    "\n",
    "df_filtered = df[df['duration'] < max_duration][['id', 'duration', 'bike_model']]\n",
    "\n",
    "# Bin size (in seconds)\n",
    "bins = range(0, df_filtered['duration'].max() + 60, 60) \n",
    "\n",
    "labels = [f\"{i}-{i+9}\" for i in bins[:-1]]\n",
    "\n",
    "df_filtered['duration_bin'] = pd.cut(df_filtered['duration'], bins=bins, right=False)\n",
    "\n",
    "duration_counts = df_filtered.groupby(['duration_bin', 'bike_model']).size().unstack(fill_value=0)\n",
    "\n",
    "duration_counts_df = duration_counts.reset_index()\n",
    "\n",
    "# Interval start\n",
    "duration_counts_df['interval_start'] = duration_counts_df['duration_bin'].apply(lambda x: int(x.left))\n",
    "\n",
    "# Normalize the \"EFIT\" column\n",
    "total_efit = duration_counts_df['EFIT'].sum()\n",
    "duration_counts_df['EFIT_normalized'] = duration_counts_df['EFIT'] / total_efit\n",
    "\n",
    "# Normalize the \"ICONIC\" column\n",
    "total_iconic = duration_counts_df['ICONIC'].sum()\n",
    "duration_counts_df['ICONIC_normalized'] = duration_counts_df['ICONIC'] / total_iconic\n",
    "\n",
    "duration_counts_df.to_csv('../../static/duration_counts.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duration stat table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  bike_model     mean  median      std  min      max\n",
      "0     ICONIC  1086.91   720.0  8078.08    0  2148980\n",
      "1       EFIT  1091.16   747.0  4482.35    0   870452\n"
     ]
    }
   ],
   "source": [
    "# Filter for specific bike models\n",
    "filtered_models = ['ICONIC', 'EFIT']\n",
    "filtered_df = df[df['bike_model'].isin(filtered_models)]\n",
    "\n",
    "# Get unique bike models from the filtered DataFrame\n",
    "bike_models = filtered_df['bike_model'].unique()\n",
    "\n",
    "# Calculate statistics from filtered DataFrame\n",
    "stats_filtered = filtered_df.groupby('bike_model')['duration'].agg(['mean', 'median', 'std']).reset_index()\n",
    "\n",
    "# Calculate min and max from original DataFrame\n",
    "stats_original = df[df['bike_model'].isin(filtered_models)].groupby('bike_model')['duration'].agg(['min', 'max']).reset_index()\n",
    "\n",
    "# Merge the statistics\n",
    "stats = pd.merge(stats_filtered, stats_original, on='bike_model')\n",
    "\n",
    "# Round statistics to 2 decimal places\n",
    "stats = stats.round(2)\n",
    "\n",
    "# Sort statistics to show ICONIC first\n",
    "stats = stats.set_index('bike_model').loc[['ICONIC', 'EFIT']].reset_index()\n",
    "\n",
    "# Display the statistics\n",
    "print(stats)\n",
    "\n",
    "stats.to_csv('../../static/duration_stats.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance histogram data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/69/g590bg750s935v88zgfrdm9w0000gn/T/ipykernel_60084/2752816429.py:13: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  distance_counts = df_filtered.groupby(['distance_bin', 'bike_model']).size().unstack(fill_value=0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>bike_model</th>\n",
       "      <th>distance_bin</th>\n",
       "      <th>EFIT</th>\n",
       "      <th>ICONIC</th>\n",
       "      <th>interval_start</th>\n",
       "      <th>EFIT_normalized</th>\n",
       "      <th>ICONIC_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 100)</td>\n",
       "      <td>126</td>\n",
       "      <td>498</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>0.000792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[100, 200)</td>\n",
       "      <td>238</td>\n",
       "      <td>1451</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001809</td>\n",
       "      <td>0.002308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[200, 300)</td>\n",
       "      <td>418</td>\n",
       "      <td>2803</td>\n",
       "      <td>200</td>\n",
       "      <td>0.003178</td>\n",
       "      <td>0.004458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[300, 400)</td>\n",
       "      <td>682</td>\n",
       "      <td>4798</td>\n",
       "      <td>300</td>\n",
       "      <td>0.005185</td>\n",
       "      <td>0.007630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[400, 500)</td>\n",
       "      <td>1009</td>\n",
       "      <td>7766</td>\n",
       "      <td>400</td>\n",
       "      <td>0.007671</td>\n",
       "      <td>0.012350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>[11500, 11600)</td>\n",
       "      <td>69</td>\n",
       "      <td>132</td>\n",
       "      <td>11500</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>0.000210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>[11600, 11700)</td>\n",
       "      <td>64</td>\n",
       "      <td>132</td>\n",
       "      <td>11600</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>0.000210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>[11700, 11800)</td>\n",
       "      <td>52</td>\n",
       "      <td>103</td>\n",
       "      <td>11700</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>0.000164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>[11800, 11900)</td>\n",
       "      <td>61</td>\n",
       "      <td>112</td>\n",
       "      <td>11800</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>0.000178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>[11900, 12000)</td>\n",
       "      <td>56</td>\n",
       "      <td>132</td>\n",
       "      <td>11900</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>0.000210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "bike_model    distance_bin  EFIT  ICONIC interval_start  EFIT_normalized  \\\n",
       "0                 [0, 100)   126     498              0         0.000958   \n",
       "1               [100, 200)   238    1451            100         0.001809   \n",
       "2               [200, 300)   418    2803            200         0.003178   \n",
       "3               [300, 400)   682    4798            300         0.005185   \n",
       "4               [400, 500)  1009    7766            400         0.007671   \n",
       "..                     ...   ...     ...            ...              ...   \n",
       "115         [11500, 11600)    69     132          11500         0.000525   \n",
       "116         [11600, 11700)    64     132          11600         0.000487   \n",
       "117         [11700, 11800)    52     103          11700         0.000395   \n",
       "118         [11800, 11900)    61     112          11800         0.000464   \n",
       "119         [11900, 12000)    56     132          11900         0.000426   \n",
       "\n",
       "bike_model  ICONIC_normalized  \n",
       "0                    0.000792  \n",
       "1                    0.002308  \n",
       "2                    0.004458  \n",
       "3                    0.007630  \n",
       "4                    0.012350  \n",
       "..                        ...  \n",
       "115                  0.000210  \n",
       "116                  0.000210  \n",
       "117                  0.000164  \n",
       "118                  0.000178  \n",
       "119                  0.000210  \n",
       "\n",
       "[120 rows x 6 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering for trips less n meters\n",
    "max_distance = 12000 \n",
    "\n",
    "df_filtered = df[df['distance_average'] < max_distance][['id', 'distance_average', 'bike_model']]\n",
    "\n",
    "# Bin size (in meters)\n",
    "bins = range(0, int(df_filtered['distance_average'].max()) + 100, 100) \n",
    "\n",
    "labels = [f\"{i}-{i+9}\" for i in bins[:-1]]\n",
    "\n",
    "df_filtered['distance_bin'] = pd.cut(df_filtered['distance_average'], bins=bins, right=False)\n",
    "\n",
    "distance_counts = df_filtered.groupby(['distance_bin', 'bike_model']).size().unstack(fill_value=0)\n",
    "\n",
    "distance_counts_df = distance_counts.reset_index()\n",
    "\n",
    "# Interval start\n",
    "distance_counts_df['interval_start'] = distance_counts_df['distance_bin'].apply(lambda x: int(x.left))\n",
    "\n",
    "# Normalize the \"EFIT\" column\n",
    "total_efit = distance_counts_df['EFIT'].sum()\n",
    "distance_counts_df['EFIT_normalized'] = distance_counts_df['EFIT'] / total_efit\n",
    "\n",
    "# Normalize the \"ICONIC\" column\n",
    "total_iconic = distance_counts_df['ICONIC'].sum()\n",
    "distance_counts_df['ICONIC_normalized'] = distance_counts_df['ICONIC'] / total_iconic\n",
    "\n",
    "distance_counts_df.to_csv('../../static/distance_counts.csv', index=False)\n",
    "\n",
    "distance_counts_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance stat table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  bike_model     mean   median      std  min      max\n",
      "0     ICONIC  2768.05  2303.09  1944.19  0.0  29671.7\n",
      "1       EFIT  3390.23  2806.44  2391.33  0.0  27845.2\n"
     ]
    }
   ],
   "source": [
    "# Filter for specific bike models\n",
    "filtered_models = ['ICONIC', 'EFIT']\n",
    "filtered_df = df[df['bike_model'].isin(filtered_models)]\n",
    "\n",
    "# Get unique bike models from the filtered DataFrame\n",
    "bike_models = filtered_df['bike_model'].unique()\n",
    "\n",
    "# Calculate statistics from filtered DataFrame\n",
    "stats_filtered = filtered_df.groupby('bike_model')['distance_average'].agg(['mean', 'median', 'std']).reset_index()\n",
    "\n",
    "# Calculate min and max from original DataFrame\n",
    "stats_original = df[df['bike_model'].isin(filtered_models)].groupby('bike_model')['distance_average'].agg(['min', 'max']).reset_index()\n",
    "\n",
    "# Merge the statistics\n",
    "stats = pd.merge(stats_filtered, stats_original, on='bike_model')\n",
    "\n",
    "# Round statistics to 2 decimal places\n",
    "stats = stats.round(2)\n",
    "\n",
    "# Sort statistics to show ICONIC first\n",
    "stats = stats.set_index('bike_model').loc[['ICONIC', 'EFIT']].reset_index()\n",
    "\n",
    "# Display the statistics\n",
    "print(stats)\n",
    "\n",
    "stats.to_csv('../../static/distance_stats.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elevation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elevation histogram data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/69/g590bg750s935v88zgfrdm9w0000gn/T/ipykernel_60084/1855856204.py:16: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  elevation_counts = df_filtered.groupby(['elevation_bin', 'bike_model']).size().unstack(fill_value=0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>bike_model</th>\n",
       "      <th>elevation_bin</th>\n",
       "      <th>EFIT</th>\n",
       "      <th>ICONIC</th>\n",
       "      <th>interval_start</th>\n",
       "      <th>EFIT_normalized</th>\n",
       "      <th>ICONIC_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-109, -107)</td>\n",
       "      <td>12</td>\n",
       "      <td>82</td>\n",
       "      <td>-109</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-107, -105)</td>\n",
       "      <td>15</td>\n",
       "      <td>57</td>\n",
       "      <td>-107</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-105, -103)</td>\n",
       "      <td>15</td>\n",
       "      <td>134</td>\n",
       "      <td>-105</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-103, -101)</td>\n",
       "      <td>27</td>\n",
       "      <td>222</td>\n",
       "      <td>-103</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.000353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-101, -99)</td>\n",
       "      <td>32</td>\n",
       "      <td>181</td>\n",
       "      <td>-101</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.000288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>[99, 101)</td>\n",
       "      <td>23</td>\n",
       "      <td>142</td>\n",
       "      <td>99</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.000226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>[101, 103)</td>\n",
       "      <td>17</td>\n",
       "      <td>164</td>\n",
       "      <td>101</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>[103, 105)</td>\n",
       "      <td>18</td>\n",
       "      <td>77</td>\n",
       "      <td>103</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>[105, 107)</td>\n",
       "      <td>7</td>\n",
       "      <td>34</td>\n",
       "      <td>105</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>[107, 109)</td>\n",
       "      <td>6</td>\n",
       "      <td>68</td>\n",
       "      <td>107</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "bike_model elevation_bin  EFIT  ICONIC interval_start  EFIT_normalized  \\\n",
       "0           [-109, -107)    12      82           -109         0.000091   \n",
       "1           [-107, -105)    15      57           -107         0.000113   \n",
       "2           [-105, -103)    15     134           -105         0.000113   \n",
       "3           [-103, -101)    27     222           -103         0.000204   \n",
       "4            [-101, -99)    32     181           -101         0.000242   \n",
       "..                   ...   ...     ...            ...              ...   \n",
       "104            [99, 101)    23     142             99         0.000174   \n",
       "105           [101, 103)    17     164            101         0.000128   \n",
       "106           [103, 105)    18      77            103         0.000136   \n",
       "107           [105, 107)     7      34            105         0.000053   \n",
       "108           [107, 109)     6      68            107         0.000045   \n",
       "\n",
       "bike_model  ICONIC_normalized  \n",
       "0                    0.000130  \n",
       "1                    0.000091  \n",
       "2                    0.000213  \n",
       "3                    0.000353  \n",
       "4                    0.000288  \n",
       "..                        ...  \n",
       "104                  0.000226  \n",
       "105                  0.000261  \n",
       "106                  0.000122  \n",
       "107                  0.000054  \n",
       "108                  0.000108  \n",
       "\n",
       "[109 rows x 6 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_elevation = 110\n",
    "min_elevation = -110 \n",
    "\n",
    "# max_elevation = df['elevation_delta_average'].max()\n",
    "# min_elevation = df['elevation_delta_average'].min()\n",
    "\n",
    "df_filtered = df[(df['elevation_delta_average'] < max_elevation) & (df['elevation_delta_average'] > min_elevation)][['id', 'elevation_delta_average', 'bike_model']]\n",
    "\n",
    "# Bin size (in meters)\n",
    "bins = range(int(df_filtered['elevation_delta_average'].min()), int(df_filtered['elevation_delta_average'].max()) + 2, 2) \n",
    "\n",
    "labels = [f\"{i}-{i+9}\" for i in bins[:-1]]\n",
    "\n",
    "df_filtered['elevation_bin'] = pd.cut(df_filtered['elevation_delta_average'], bins=bins, right=False)\n",
    "\n",
    "elevation_counts = df_filtered.groupby(['elevation_bin', 'bike_model']).size().unstack(fill_value=0)\n",
    "\n",
    "elevation_counts_df = elevation_counts.reset_index()\n",
    "\n",
    "elevation_counts_df['interval_start'] = elevation_counts_df['elevation_bin'].apply(lambda x: int(x.left))\n",
    "\n",
    "# Normalize the \"EFIT\" column\n",
    "total_efit = elevation_counts_df['EFIT'].sum()\n",
    "elevation_counts_df['EFIT_normalized'] = elevation_counts_df['EFIT'] / total_efit\n",
    "\n",
    "# Normalize the \"ICONIC\" column\n",
    "total_iconic = elevation_counts_df['ICONIC'].sum()\n",
    "elevation_counts_df['ICONIC_normalized'] = elevation_counts_df['ICONIC'] / total_iconic\n",
    "\n",
    "elevation_counts_df.to_csv('../../static/elevation_counts.csv', index=False)\n",
    "\n",
    "elevation_counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elevation stat table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  bike_model  mean  median    std  abs_mean  abs_median    min    max\n",
      "0     ICONIC -1.35    -0.5  21.49     14.69        10.0 -179.5  182.0\n",
      "1       EFIT -0.89     0.0  24.81     17.85        13.0 -173.0  179.5\n"
     ]
    }
   ],
   "source": [
    "# Filter for specific bike models\n",
    "filtered_models = ['ICONIC', 'EFIT']\n",
    "filtered_df = df[df['bike_model'].isin(filtered_models)]\n",
    "\n",
    "# Get unique bike models from the filtered DataFrame\n",
    "bike_models = filtered_df['bike_model'].unique()\n",
    "\n",
    "# Calculate statistics from filtered DataFrame\n",
    "stats_filtered = filtered_df.groupby('bike_model')['elevation_delta_average'].agg(['mean', 'median', 'std']).reset_index()\n",
    "\n",
    "# Calculate absolute mean and absolute median\n",
    "stats_filtered['abs_mean'] = filtered_df.groupby('bike_model')['elevation_delta_average'].apply(lambda x: x.abs().mean()).values\n",
    "stats_filtered['abs_median'] = filtered_df.groupby('bike_model')['elevation_delta_average'].apply(lambda x: x.abs().median()).values\n",
    "\n",
    "# Calculate min and max from original DataFrame\n",
    "stats_original = df[df['bike_model'].isin(filtered_models)].groupby('bike_model')['elevation_delta_average'].agg(['min', 'max']).reset_index()\n",
    "\n",
    "# Merge the statistics\n",
    "stats = pd.merge(stats_filtered, stats_original, on='bike_model')\n",
    "\n",
    "# Round statistics to 2 decimal places\n",
    "stats = stats.round(2)\n",
    "\n",
    "# Sort statistics to show ICONIC first\n",
    "stats = stats.set_index('bike_model').loc[['ICONIC', 'EFIT']].reset_index()\n",
    "\n",
    "# Display the statistics\n",
    "print(stats)\n",
    "\n",
    "stats.to_csv('../../static/elevation_stats.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radial data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 24 hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bike_model half_hour_intervals  EFIT  ICONIC  EFIT_normalized  \\\n",
      "0                        00:00  1228    5208         0.009250   \n",
      "1                        00:30  1023    3878         0.007705   \n",
      "2                        01:00   753    3083         0.005672   \n",
      "3                        01:30   680    2596         0.005122   \n",
      "4                        02:00   607    2358         0.004572   \n",
      "5                        02:30   528    1984         0.003977   \n",
      "6                        03:00   391    1329         0.002945   \n",
      "7                        03:30   283     830         0.002132   \n",
      "8                        04:00   200     730         0.001506   \n",
      "9                        04:30   247     848         0.001860   \n",
      "10                       05:00   349    1183         0.002629   \n",
      "11                       05:30   600    2139         0.004519   \n",
      "12                       06:00   763    3010         0.005747   \n",
      "13                       06:30  1427    5947         0.010748   \n",
      "14                       07:00  1730    7635         0.013031   \n",
      "15                       07:30  2480   11950         0.018680   \n",
      "16                       08:00  3209   16742         0.024171   \n",
      "17                       08:30  3926   21078         0.029571   \n",
      "18                       09:00  3091   14397         0.023282   \n",
      "19                       09:30  2807   12696         0.021143   \n",
      "20                       10:00  2527   11016         0.019034   \n",
      "21                       10:30  2749   12389         0.020706   \n",
      "22                       11:00  2729   12172         0.020555   \n",
      "23                       11:30  3274   14644         0.024660   \n",
      "24                       12:00  3483   14623         0.026235   \n",
      "25                       12:30  3688   15937         0.027779   \n",
      "26                       13:00  3851   15692         0.029007   \n",
      "27                       13:30  3939   15943         0.029669   \n",
      "28                       14:00  4051   16194         0.030513   \n",
      "29                       14:30  4355   17205         0.032803   \n",
      "30                       15:00  4638   18086         0.034934   \n",
      "31                       15:30  4880   19042         0.036757   \n",
      "32                       16:00  5451   22364         0.041058   \n",
      "33                       16:30  5652   26543         0.042572   \n",
      "34                       17:00  6619   34852         0.049856   \n",
      "35                       17:30  6140   34249         0.046248   \n",
      "36                       18:00  5378   31609         0.040508   \n",
      "37                       18:30  4851   27845         0.036539   \n",
      "38                       19:00  4343   25051         0.032712   \n",
      "39                       19:30  3927   22171         0.029579   \n",
      "40                       20:00  3453   19929         0.026009   \n",
      "41                       20:30  3159   17409         0.023794   \n",
      "42                       21:00  2844   15530         0.021422   \n",
      "43                       21:30  2532   13189         0.019072   \n",
      "44                       22:00  2228   11222         0.016782   \n",
      "45                       22:30  2054    9652         0.015471   \n",
      "46                       23:00  2090    9466         0.015742   \n",
      "47                       23:30  1556    7017         0.011720   \n",
      "\n",
      "bike_model  ICONIC_normalized  \n",
      "0                    0.008258  \n",
      "1                    0.006149  \n",
      "2                    0.004889  \n",
      "3                    0.004116  \n",
      "4                    0.003739  \n",
      "5                    0.003146  \n",
      "6                    0.002107  \n",
      "7                    0.001316  \n",
      "8                    0.001158  \n",
      "9                    0.001345  \n",
      "10                   0.001876  \n",
      "11                   0.003392  \n",
      "12                   0.004773  \n",
      "13                   0.009430  \n",
      "14                   0.012106  \n",
      "15                   0.018948  \n",
      "16                   0.026547  \n",
      "17                   0.033422  \n",
      "18                   0.022828  \n",
      "19                   0.020131  \n",
      "20                   0.017467  \n",
      "21                   0.019644  \n",
      "22                   0.019300  \n",
      "23                   0.023220  \n",
      "24                   0.023187  \n",
      "25                   0.025270  \n",
      "26                   0.024882  \n",
      "27                   0.025280  \n",
      "28                   0.025678  \n",
      "29                   0.027281  \n",
      "30                   0.028678  \n",
      "31                   0.030194  \n",
      "32                   0.035461  \n",
      "33                   0.042088  \n",
      "34                   0.055263  \n",
      "35                   0.054306  \n",
      "36                   0.050120  \n",
      "37                   0.044152  \n",
      "38                   0.039722  \n",
      "39                   0.035155  \n",
      "40                   0.031600  \n",
      "41                   0.027604  \n",
      "42                   0.024625  \n",
      "43                   0.020913  \n",
      "44                   0.017794  \n",
      "45                   0.015305  \n",
      "46                   0.015010  \n",
      "47                   0.011126  \n"
     ]
    }
   ],
   "source": [
    "# Convert start time in seconds to half-hours\n",
    "half_hours = df['start_time_seconds'] // 1800  # 1800 seconds = 30 minutes\n",
    "\n",
    "# Add half_hours to the DataFrame\n",
    "df['half_hours'] = half_hours\n",
    "\n",
    "# Create half_hour_intervals based on half_hours\n",
    "df['half_hour_intervals'] = df['half_hours'].apply(lambda x: (pd.Timestamp('1970-01-01') + timedelta(minutes=x*30)).strftime('%H:%M'))\n",
    "\n",
    "# Group by 'bike_model' and 'half_hours', then count occurrences\n",
    "grouped = df.groupby(['bike_model', 'half_hours']).size().unstack(fill_value=0)\n",
    "\n",
    "# Normalize across each model separately\n",
    "normalized_grouped = grouped.div(grouped.sum(axis=1), axis=0)  # Normalize across rows (bike models)\n",
    "\n",
    "# Transpose to get half_hour_intervals as columns\n",
    "freq_counts = grouped.transpose()\n",
    "normalized_freq_counts = normalized_grouped.transpose()\n",
    "\n",
    "# Fix: Convert half_hours to half_hour_intervals for the resulting DataFrame\n",
    "freq_counts['half_hour_intervals'] = freq_counts.index.to_series().apply(lambda x: (pd.Timestamp('1970-01-01') + timedelta(minutes=x*30)).strftime('%H:%M'))\n",
    "normalized_freq_counts['half_hour_intervals'] = normalized_freq_counts.index.to_series().apply(lambda x: (pd.Timestamp('1970-01-01') + timedelta(minutes=x*30)).strftime('%H:%M'))\n",
    "\n",
    "# Merge the unnormalized and normalized data\n",
    "merged_freq_counts = freq_counts.merge(normalized_freq_counts, on='half_hour_intervals', suffixes=('', '_normalized'))\n",
    "\n",
    "# Rearranging columns: Move 'half_hour_intervals' to the front\n",
    "merged_freq_counts = merged_freq_counts[['half_hour_intervals'] + [col for col in merged_freq_counts.columns if col != 'half_hour_intervals']]\n",
    "\n",
    "# Output the DataFrame to check the result\n",
    "print(merged_freq_counts)\n",
    "\n",
    "# Save to CSV\n",
    "# merged_freq_counts.to_csv('../../static/freq_counts.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 24hr by day of week "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/69/g590bg750s935v88zgfrdm9w0000gn/T/ipykernel_93700/798912541.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['half_hours'] = half_hours\n",
      "/var/folders/69/g590bg750s935v88zgfrdm9w0000gn/T/ipykernel_93700/798912541.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['half_hour_intervals'] = filtered_df['half_hours'].apply(\n",
      "/var/folders/69/g590bg750s935v88zgfrdm9w0000gn/T/ipykernel_93700/798912541.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['half_hours'] = half_hours\n",
      "/var/folders/69/g590bg750s935v88zgfrdm9w0000gn/T/ipykernel_93700/798912541.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['half_hour_intervals'] = filtered_df['half_hours'].apply(\n",
      "/var/folders/69/g590bg750s935v88zgfrdm9w0000gn/T/ipykernel_93700/798912541.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['half_hours'] = half_hours\n",
      "/var/folders/69/g590bg750s935v88zgfrdm9w0000gn/T/ipykernel_93700/798912541.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['half_hour_intervals'] = filtered_df['half_hours'].apply(\n",
      "/var/folders/69/g590bg750s935v88zgfrdm9w0000gn/T/ipykernel_93700/798912541.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['half_hours'] = half_hours\n",
      "/var/folders/69/g590bg750s935v88zgfrdm9w0000gn/T/ipykernel_93700/798912541.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['half_hour_intervals'] = filtered_df['half_hours'].apply(\n",
      "/var/folders/69/g590bg750s935v88zgfrdm9w0000gn/T/ipykernel_93700/798912541.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['half_hours'] = half_hours\n",
      "/var/folders/69/g590bg750s935v88zgfrdm9w0000gn/T/ipykernel_93700/798912541.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['half_hour_intervals'] = filtered_df['half_hours'].apply(\n",
      "/var/folders/69/g590bg750s935v88zgfrdm9w0000gn/T/ipykernel_93700/798912541.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['half_hours'] = half_hours\n",
      "/var/folders/69/g590bg750s935v88zgfrdm9w0000gn/T/ipykernel_93700/798912541.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['half_hour_intervals'] = filtered_df['half_hours'].apply(\n",
      "/var/folders/69/g590bg750s935v88zgfrdm9w0000gn/T/ipykernel_93700/798912541.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['half_hours'] = half_hours\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bike_model half_hour_intervals  EFIT  ICONIC  EFIT_normalized  \\\n",
      "0                        00:00   242    1117         0.012521   \n",
      "1                        00:30   207    1013         0.010710   \n",
      "2                        01:00   210     859         0.010865   \n",
      "3                        01:30   180     866         0.009313   \n",
      "4                        02:00   170     762         0.008796   \n",
      "..                         ...   ...     ...              ...   \n",
      "331                      21:30   372    1633         0.019602   \n",
      "332                      22:00   300    1371         0.015808   \n",
      "333                      22:30   271    1185         0.014280   \n",
      "334                      23:00   322    1316         0.016967   \n",
      "335                      23:30   174     704         0.009169   \n",
      "\n",
      "bike_model  ICONIC_normalized day_of_week  \n",
      "0                    0.012603      Sunday  \n",
      "1                    0.011429      Sunday  \n",
      "2                    0.009692      Sunday  \n",
      "3                    0.009771      Sunday  \n",
      "4                    0.008597      Sunday  \n",
      "..                        ...         ...  \n",
      "331                  0.020101      Monday  \n",
      "332                  0.016876      Monday  \n",
      "333                  0.014586      Monday  \n",
      "334                  0.016199      Monday  \n",
      "335                  0.008666      Monday  \n",
      "\n",
      "[336 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/69/g590bg750s935v88zgfrdm9w0000gn/T/ipykernel_93700/798912541.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['half_hour_intervals'] = filtered_df['half_hours'].apply(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "# List to store results for each day\n",
    "results = []\n",
    "\n",
    "# Iterate over each unique day_of_week\n",
    "for day in df['day_of_week'].unique():\n",
    "    # Filter the DataFrame for the current day_of_week\n",
    "    filtered_df = df[df['day_of_week'] == day]\n",
    "    \n",
    "    # Convert start time in seconds to half-hours\n",
    "    half_hours = filtered_df['start_time_seconds'] // 1800  # 1800 seconds = 30 minutes\n",
    "\n",
    "    # Add half_hours to the filtered DataFrame\n",
    "    filtered_df['half_hours'] = half_hours\n",
    "\n",
    "    # Create half_hour_intervals based on half_hours\n",
    "    filtered_df['half_hour_intervals'] = filtered_df['half_hours'].apply(\n",
    "        lambda x: (pd.Timestamp('1970-01-01') + timedelta(minutes=x*30)).strftime('%H:%M')\n",
    "    )\n",
    "\n",
    "    # Group by 'bike_model' and 'half_hours', then count occurrences\n",
    "    grouped = filtered_df.groupby(['bike_model', 'half_hours']).size().unstack(fill_value=0)\n",
    "\n",
    "    # Normalize across each model separately\n",
    "    normalized_grouped = grouped.div(grouped.sum(axis=1), axis=0)  # Normalize across rows (bike models)\n",
    "\n",
    "    # Transpose to get half_hour_intervals as columns\n",
    "    freq_counts = grouped.transpose()\n",
    "    normalized_freq_counts = normalized_grouped.transpose()\n",
    "\n",
    "    # Convert half_hours to half_hour_intervals for the resulting DataFrame\n",
    "    freq_counts['half_hour_intervals'] = freq_counts.index.to_series().apply(\n",
    "        lambda x: (pd.Timestamp('1970-01-01') + timedelta(minutes=x*30)).strftime('%H:%M')\n",
    "    )\n",
    "    normalized_freq_counts['half_hour_intervals'] = normalized_freq_counts.index.to_series().apply(\n",
    "        lambda x: (pd.Timestamp('1970-01-01') + timedelta(minutes=x*30)).strftime('%H:%M')\n",
    "    )\n",
    "\n",
    "    # Merge the unnormalized and normalized data for this day\n",
    "    merged_freq_counts = freq_counts.merge(\n",
    "        normalized_freq_counts, on='half_hour_intervals', suffixes=('', '_normalized')\n",
    "    )\n",
    "\n",
    "    # Rearranging columns: Move 'half_hour_intervals' to the front\n",
    "    merged_freq_counts = merged_freq_counts[\n",
    "        ['half_hour_intervals'] + [col for col in merged_freq_counts.columns if col != 'half_hour_intervals']\n",
    "    ]\n",
    "\n",
    "    # Add a column to indicate the day of the week\n",
    "    merged_freq_counts['day_of_week'] = day\n",
    "\n",
    "    # Append the result for the current day to the list\n",
    "    results.append(merged_freq_counts)\n",
    "\n",
    "# Concatenate all results into a single DataFrame\n",
    "csv_result = pd.concat(results, ignore_index=True)\n",
    "print(csv_result)\n",
    "# Save to CSV\n",
    "# csv_result.to_csv('../../static/freq_counts_by_day.csv', index=False)\n",
    "\n",
    "# import json\n",
    "\n",
    "# json_result = csv_result.groupby('day_of_week').apply(lambda x: x.drop(columns='day_of_week').to_dict(orient='records')).to_dict()\n",
    "# json_result\n",
    "\n",
    "# with open('../../src/data/freq_counts_by_day.json', 'w') as json_file:\n",
    "#     json.dump(json_result, json_file, indent=4)\n",
    "\n",
    "# Output the DataFrame to check the result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 24hr by Weekday / Weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/69/g590bg750s935v88zgfrdm9w0000gn/T/ipykernel_93700/2904109490.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['half_hours'] = half_hours\n",
      "/var/folders/69/g590bg750s935v88zgfrdm9w0000gn/T/ipykernel_93700/2904109490.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['half_hour_intervals'] = filtered_df['half_hours'].apply(\n",
      "/var/folders/69/g590bg750s935v88zgfrdm9w0000gn/T/ipykernel_93700/2904109490.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['half_hours'] = half_hours\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bike_model half_hour_intervals  EFIT  ICONIC  EFIT_normalized  \\\n",
      "0                        00:00   541    2484         0.013738   \n",
      "1                        00:30   453    2041         0.011503   \n",
      "2                        01:00   414    1846         0.010513   \n",
      "3                        01:30   340    1670         0.008634   \n",
      "4                        02:00   336    1597         0.008532   \n",
      "..                         ...   ...     ...              ...   \n",
      "91                       21:30  1800    9708         0.019276   \n",
      "92                       22:00  1482    7974         0.015870   \n",
      "93                       22:30  1433    6675         0.015346   \n",
      "94                       23:00  1433    6644         0.015346   \n",
      "95                       23:30  1019    4707         0.010912   \n",
      "\n",
      "bike_model  ICONIC_normalized day_category  \n",
      "0                    0.013017      Weekend  \n",
      "1                    0.010695      Weekend  \n",
      "2                    0.009673      Weekend  \n",
      "3                    0.008751      Weekend  \n",
      "4                    0.008369      Weekend  \n",
      "..                        ...          ...  \n",
      "91                   0.022072      Weekday  \n",
      "92                   0.018130      Weekday  \n",
      "93                   0.015176      Weekday  \n",
      "94                   0.015106      Weekday  \n",
      "95                   0.010702      Weekday  \n",
      "\n",
      "[96 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/69/g590bg750s935v88zgfrdm9w0000gn/T/ipykernel_93700/2904109490.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['half_hour_intervals'] = filtered_df['half_hours'].apply(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "# List to store results for each category\n",
    "results = []\n",
    "\n",
    "# Define a function to categorize days\n",
    "def categorize_day(day):\n",
    "    if day in ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']:\n",
    "        return 'Weekday'\n",
    "    else:\n",
    "        return 'Weekend'\n",
    "\n",
    "# Add a new column for category (Weekday or Weekend)\n",
    "df['day_category'] = df['day_of_week'].apply(categorize_day)\n",
    "\n",
    "# Iterate over each unique category\n",
    "for category in df['day_category'].unique():\n",
    "    # Filter the DataFrame for the current category\n",
    "    filtered_df = df[df['day_category'] == category]\n",
    "    \n",
    "    # Convert start time in seconds to half-hours\n",
    "    half_hours = filtered_df['start_time_seconds'] // 1800  # 1800 seconds = 30 minutes\n",
    "\n",
    "    # Add half_hours to the filtered DataFrame\n",
    "    filtered_df['half_hours'] = half_hours\n",
    "\n",
    "    # Create half_hour_intervals based on half_hours\n",
    "    filtered_df['half_hour_intervals'] = filtered_df['half_hours'].apply(\n",
    "        lambda x: (pd.Timestamp('1970-01-01') + timedelta(minutes=x*30)).strftime('%H:%M')\n",
    "    )\n",
    "\n",
    "    # Group by 'bike_model' and 'half_hours', then count occurrences\n",
    "    grouped = filtered_df.groupby(['bike_model', 'half_hours']).size().unstack(fill_value=0)\n",
    "\n",
    "    # Normalize across each model separately\n",
    "    normalized_grouped = grouped.div(grouped.sum(axis=1), axis=0)  # Normalize across rows (bike models)\n",
    "\n",
    "    # Transpose to get half_hour_intervals as columns\n",
    "    freq_counts = grouped.transpose()\n",
    "    normalized_freq_counts = normalized_grouped.transpose()\n",
    "\n",
    "    # Convert half_hours to half_hour_intervals for the resulting DataFrame\n",
    "    freq_counts['half_hour_intervals'] = freq_counts.index.to_series().apply(\n",
    "        lambda x: (pd.Timestamp('1970-01-01') + timedelta(minutes=x*30)).strftime('%H:%M')\n",
    "    )\n",
    "    normalized_freq_counts['half_hour_intervals'] = normalized_freq_counts.index.to_series().apply(\n",
    "        lambda x: (pd.Timestamp('1970-01-01') + timedelta(minutes=x*30)).strftime('%H:%M')\n",
    "    )\n",
    "\n",
    "    # Merge the unnormalized and normalized data for this category\n",
    "    merged_freq_counts = freq_counts.merge(\n",
    "        normalized_freq_counts, on='half_hour_intervals', suffixes=('', '_normalized')\n",
    "    )\n",
    "\n",
    "    # Rearranging columns: Move 'half_hour_intervals' to the front\n",
    "    merged_freq_counts = merged_freq_counts[\n",
    "        ['half_hour_intervals'] + [col for col in merged_freq_counts.columns if col != 'half_hour_intervals']\n",
    "    ]\n",
    "\n",
    "    # Add a column to indicate the category (Weekday or Weekend)\n",
    "    merged_freq_counts['day_category'] = category\n",
    "\n",
    "    # Append the result for the current category to the list\n",
    "    results.append(merged_freq_counts)\n",
    "\n",
    "# Concatenate all results into a single DataFrame\n",
    "csv_result = pd.concat(results, ignore_index=True)\n",
    "print(csv_result)\n",
    "\n",
    "# Save to CSV\n",
    "csv_result.to_csv('../../static/freq_counts_by_day_category.csv', index=False)\n",
    "\n",
    "# Optional: Convert to JSON format\n",
    "# json_result = csv_result.groupby('day_category').apply(\n",
    "#     lambda x: x.drop(columns='day_category').to_dict(orient='records')\n",
    "# ).to_dict()\n",
    "\n",
    "# Save the JSON result\n",
    "# with open('../../src/data/freq_counts_by_day.json', 'w') as json_file:\n",
    "#     json.dump(json_result, json_file, indent=4)\n",
    "\n",
    "# Output the DataFrame to check the result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
